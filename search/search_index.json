{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"SwarmX","text":"<p>An extreme simple framework exploring ergonomic, lightweight multi-agent orchestration.</p>"},{"location":"#highlights","title":"Highlights","text":"<ol> <li>SwarmX is both Agent and Workflow</li> <li>MCP servers support</li> <li>OpenAI-compatible streaming-server</li> <li>Workflow import/export in JSON format</li> </ol>"},{"location":"#json-format-details","title":"JSON Format Details","text":"<p>SwarmX supports importing and exporting workflows in JSON format. The JSON structure includes: - <code>nodes</code>: List of nodes with their type (<code>agent</code> or <code>swarm</code>) and associated data - <code>edges</code>: List of edges connecting nodes, optionally with conditions - <code>mcpServers</code>: Optional configuration for MCP servers</p> <p>Example JSON structure: <pre><code>{\n  \"nodes\": [\n    {\n      \"id\": 0,\n      \"type\": \"agent\",\n      \"agent\": {\n        \"name\": \"Assistant\",\n        \"model\": \"gpt-4o\",\n        \"instructions\": \"You are a helpful agent.\"\n      }\n    },\n    {\n      \"id\": 1,\n      \"type\": \"agent\",\n      \"agent\": {\n        \"name\": \"Specialist\",\n        \"model\": \"deepseek-r1:7b\",\n        \"instructions\": \"You are a specialist agent.\"\n      }\n    }\n  ],\n  \"edges\": [\n    {\n      \"source\": 0,\n      \"target\": 1\n    }\n  ]\n}\n</code></pre></p>"},{"location":"#star-history","title":"Star History","text":""},{"location":"#quick-start","title":"Quick start","text":"<p>SwarmX automatically loads environment variables from a <code>.env</code> file if present. You can either:</p> <ol> <li> <p>Use a .env file (recommended):    <pre><code># Create a .env file in your project directory\necho \"OPENAI_API_KEY=your-api-key\" &gt; .env\necho \"OPENAI_BASE_URL=http://localhost:11434/v1\" &gt;&gt; .env  # optional\nuvx swarmx  # Start interactive REPL\n</code></pre></p> </li> <li> <p>Set environment variables manually:    <pre><code>export OPENAI_API_KEY=\"your-api-key\"\n# export OPENAI_BASE_URL=\"http://localhost:11434/v1\"  # optional\nuvx swarmx  # Start interactive REPL\n</code></pre></p> </li> </ol>"},{"location":"#api-server","title":"API Server","text":"<p>You can also start SwarmX as an OpenAI-compatible API server:</p> <pre><code>uvx swarmx serve --host 0.0.0.0 --port 8000\n</code></pre> <p>This provides OpenAI-compatible endpoints:</p> <ul> <li><code>POST /v1/chat/completions</code> - Chat completions with streaming support</li> <li><code>GET /v1/models</code> - List available models</li> </ul> <p>Use it with any OpenAI-compatible client:</p> <pre><code>import openai\n\nclient = openai.OpenAI(\n    base_url=\"http://localhost:8000/v1\",\n    api_key=\"dummy\"  # SwarmX doesn't require authentication\n)\n\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n</code></pre>"},{"location":"#installation","title":"Installation","text":"<p>Requires Python 3.11+</p> <pre><code>$ pip install swarmx # or `uv tool install swarmx`\n</code></pre>"},{"location":"#usage","title":"Usage","text":"<pre><code>import asyncio\nfrom swarmx import Swarm, Agent\n\nclient = Swarm()\n\ndef transfer_to_agent_b():\n    return agent_b\n\n\nagent_a = Agent(\n    name=\"Agent A\",\n    instructions=\"You are a helpful agent.\",\n    functions=[transfer_to_agent_b],\n)\n\nagent_b = Agent(\n    name=\"Agent B\",\n    model=\"deepseek-r1:7b\",\n    instructions=\"\u4f60\u53ea\u80fd\u8bf4\u4e2d\u6587\u3002\",  # You can only speak Chinese.\n)\n\n\nasync def main():\n    response = await client.run(\n        agent=agent_a,\n        messages=[{\"role\": \"user\", \"content\": \"I want to talk to agent B.\"}],\n    )\n\n    print(response.messages[-1][\"content\"])\n\n\nasyncio.run(main())\n</code></pre>"},{"location":"#architecture","title":"Architecture","text":"<pre><code>graph TD\n   classDef QA fill:#ffffff;\n   classDef agent fill:#ffd8ac;\n   classDef tool fill:#d3ecee;\n   classDef result fill:#b4f2be;\n   func1(\"transfer_to_weather_assistant()\"):::tool\n   Weather[\"Weather Assistant\"]:::agent\n   func2(\"get_weather('New York')\"):::tool\n   temp(64):::result\n   A[\"It's 64 degrees in New York.\"]:::QA\n   Q[\"What's the weather in ny?\"]:::QA --&gt; \n   Triage[\"Triage Agent\"]:::agent --&gt; Weather --&gt; A\n   Triage --&gt; func1 --&gt; Weather\n   Weather --&gt; func2 --&gt; temp --&gt; A</code></pre>"},{"location":"hooks/","title":"SwarmX Hooks","text":"<p>SwarmX supports lifecycle hooks that allow you to execute MCP tools at specific points during agent execution. This enables powerful capabilities like logging, metrics collection, debugging, and custom workflow orchestration.</p>"},{"location":"hooks/#hook-overview","title":"Hook Overview","text":"<p>A <code>Hook</code> is a Pydantic BaseModel that defines which MCP tools to execute at various lifecycle events:</p> <ul> <li><code>on_start</code>: Executed when the agent begins processing</li> <li><code>on_end</code>: Executed when the agent finishes processing  </li> <li><code>on_tool_start</code>: Executed before any tool call</li> <li><code>on_tool_end</code>: Executed after any tool call</li> <li><code>on_subagents_start</code>: Executed before subagent processing begins</li> <li><code>on_subagents_end</code>: Executed after subagent processing ends</li> </ul>"},{"location":"hooks/#creating-hooks","title":"Creating Hooks","text":"<pre><code>from swarmx import Hook\n\n# Create a logging hook\nlogging_hook = Hook(\n    on_start=\"log_agent_start\",\n    on_end=\"log_agent_end\",\n    on_tool_start=\"log_tool_start\", \n    on_tool_end=\"log_tool_end\"\n)\n\n# Create a metrics hook\nmetrics_hook = Hook(\n    on_start=\"start_timer\",\n    on_end=\"record_execution_time\"\n)\n</code></pre>"},{"location":"hooks/#adding-hooks-to-agents","title":"Adding Hooks to Agents","text":"<pre><code>from swarmx import Agent, Hook\n\n# Create hooks\nhook1 = Hook(on_start=\"initialize_session\")\nhook2 = Hook(on_end=\"cleanup_session\")\n\n# Add hooks to agent\nagent = Agent(\n    name=\"MyAgent\",\n    hooks=[hook1, hook2]\n)\n</code></pre>"},{"location":"hooks/#hook-execution","title":"Hook Execution","text":"<p>Hooks are executed automatically during agent lifecycle:</p> <ol> <li>on_start - Called at the beginning of <code>_run()</code> and <code>_run_stream()</code></li> <li>on_tool_start - Called before executing any tool calls</li> <li>on_tool_end - Called after executing tool calls</li> <li>on_subagents_start - Called before running subagents</li> <li>on_subagents_end - Called after running subagents  </li> <li>on_end - Called at the end of processing</li> </ol>"},{"location":"hooks/#hook-tool-requirements","title":"Hook Tool Requirements","text":"<p>Hook tools must be available in your MCP server configuration. The tools receive both the current messages and context as input parameters, and can return modified versions through structured output.</p>"},{"location":"hooks/#input-format","title":"Input Format","text":"<p>Hook tools receive input in this format: <pre><code>{\n    \"messages\": [...],  # Current conversation messages\n    \"context\": {...}    # Agent context (can be None)\n}\n</code></pre></p>"},{"location":"hooks/#output-format","title":"Output Format","text":"<p>Hook tools can return structured output to modify messages and context: <pre><code>{\n    \"messages\": [...],  # Modified messages\n    \"context\": {...}    # Modified context (can be None)\n}\n</code></pre></p> <p>If no modifications are needed, return the input unchanged.</p>"},{"location":"hooks/#example-mcp-tools","title":"Example MCP Tools","text":"<pre><code># In your MCP server\n@server.call_tool()\nasync def log_agent_start(input_data: dict) -&gt; dict:\n    \"\"\"Log when an agent starts processing.\"\"\"\n    messages = input_data[\"messages\"]\n    context = input_data[\"context\"]\n\n    logger.info(f\"Agent started with {len(messages)} messages and context: {context}\")\n\n    # Return unchanged if no modifications needed\n    return {\n        \"messages\": messages,\n        \"context\": context\n    }\n\n@server.call_tool()\nasync def add_system_context(input_data: dict) -&gt; dict:\n    \"\"\"Add system information to context.\"\"\"\n    messages = input_data[\"messages\"]\n    context = input_data.get(\"context\") or {}\n\n    # Modify context\n    context[\"system_info\"] = {\n        \"timestamp\": datetime.now().isoformat(),\n        \"message_count\": len(messages)\n    }\n\n    return {\n        \"messages\": messages,\n        \"context\": context\n    }\n\n@server.call_tool()\nasync def filter_messages(input_data: dict) -&gt; dict:\n    \"\"\"Filter out certain message types.\"\"\"\n    messages = input_data[\"messages\"]\n    context = input_data[\"context\"]\n\n    # Filter messages\n    filtered_messages = [\n        msg for msg in messages\n        if msg.get(\"role\") != \"system\" or \"important\" in msg.get(\"content\", \"\")\n    ]\n\n    return {\n        \"messages\": filtered_messages,\n        \"context\": context\n    }\n</code></pre>"},{"location":"hooks/#error-handling","title":"Error Handling","text":"<p>If a hook tool fails, the error is logged but does not stop agent execution. This ensures that hook failures don't break your main workflow.</p>"},{"location":"hooks/#serialization","title":"Serialization","text":"<p>Hooks are fully serializable since they only store tool names (strings) rather than function references:</p> <pre><code># Serialize\nhook_dict = hook.model_dump()\n\n# Deserialize  \nrestored_hook = Hook.model_validate(hook_dict)\n</code></pre>"},{"location":"hooks/#use-cases","title":"Use Cases","text":"<ul> <li>Logging: Track agent execution flow</li> <li>Metrics: Measure performance and usage</li> <li>Debugging: Inspect agent state at key points</li> <li>Auditing: Record all agent actions</li> <li>Integration: Trigger external systems</li> <li>Workflow: Coordinate complex multi-agent processes</li> </ul>"},{"location":"hooks/#example","title":"Example","text":"<p>See <code>examples/hooks_example.py</code> for a complete working example.</p>"},{"location":"server/","title":"OpenAI\u2011compatible Server","text":"<p><code>swarmx.server</code> provides a FastAPI application exposing OpenAI\u2011compatible endpoints:</p> <ul> <li>GET\u202f/v1/models \u2013 lists all agents in the swarm as models.</li> <li>POST\u202f/v1/chat/completions \u2013 handles chat requests with optional streaming.</li> </ul>"},{"location":"server/#streaming-semantics","title":"Streaming semantics","text":"<p>The server streams responses using the same chunk format as the internal <code>Agent._run_stream</code> implementation. Important points:</p> <ol> <li>Multiple concurrent streams \u2013 each request creates its own independent stream. Streams are ordered internally; the client receives chunks in the order they are produced.</li> <li>Chunk IDs \u2013 every chunk carries an <code>id</code> that uniquely identifies the message it belongs to. When merging chunks into a final message, the order of the resulting message follows the order of chunks with the same <code>id</code> as they appear in the stream.</li> <li>First chunk of each stream \u2013 the first chunk for a given <code>id</code> is always emitted before any subsequent chunks for that <code>id</code>, guaranteeing that the message can be reconstructed incrementally.</li> </ol> <p>This behavior mirrors the OpenAI API contract, allowing clients to consume streamed data safely while the server internally aggregates chunks into complete messages.</p>"},{"location":"server/#usage-example","title":"Usage example","text":"<pre><code>uvx swarmx serve --host 0.0.0.0 --port 8000\n</code></pre> <pre><code>import openai\nclient = openai.OpenAI(base_url=\"http://localhost:8000/v1\", api_key=\"dummy\")\nresp = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}],\n    stream=True,\n)\nfor chunk in resp:\n    print(chunk.choices[0].delta.content, end=\"\")\n</code></pre> <p>The above script will print the assistant\u2019s reply as it arrives, respecting the ordering guarantees described.</p>"}]}